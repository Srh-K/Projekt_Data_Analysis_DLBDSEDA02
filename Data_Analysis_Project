import re
import nltk
import pandas as pd
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import itertools

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords, wordnet
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation
from sklearn.model_selection import GridSearchCV

import gensim
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel
from gensim.matutils import Sparse2Corpus
from gensim.models.ldamodel import LdaModel

from tabulate import tabulate
from keybert import KeyBERT
from scipy.sparse import csr_matrix


# Laden der NLTK-Ressourcen
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')


# Inhalt der CSV-Datei lesen und die ersten 4000 Zeilen berücksichtigen
csv_path = 'customer-complaints.csv'
df = pd.read_csv(csv_path, nrows=4000)

#Ausgabe der ersten 10 Beschwerden
print("Ausgabe der ersten 10 Beschwerden vor der Bereinigung")
issues = df['Issue'][0:10]
for idx, issue in enumerate(issues, start=1):
    print(f"{idx}: {issue}")

# Bereinigung der Beschwerde-Daten
# Entfernung der Sonderzeichen (außer Apostrophe) und "'s" 
df['Issue'] = df['Issue'].apply(lambda x: re.sub(r"'s\b", "", re.sub(r"[^\w\s']", '', x)))

# Schritt 1: Konvertierung des Textes in Kleinbuchstaben
df['Issue'] = df['Issue'].str.lower()

# Schritt 2: Tokenisierung des Textes
df['Issue'] = df['Issue'].apply(lambda x: word_tokenize(str(x)))

# Schritt 3: Entfernung der englischen Stoppwörter / Erweiterung der Stoppwortliste um "ca n't"
stop_words = set(stopwords.words("english")).union({"ca", "n't"})
df['Issue'] = df['Issue'].apply(lambda x: [token for token in x if token not in stop_words])


# Funktion zum Bestimmen des POS-Tags
def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

# Funktion zur Lemmatisierung mit Berücksichtigung des POS-Tags
def lemmatize_text(tokens):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]
df['Issue'] = df['Issue'].apply(lemmatize_text)

#Ausgabe der ersten 10 Beschwerden nach der Bereinigung
print("\n\nAusgabe der ersten 10 Beschwerden nach der Bereinigung")
issues = df['Issue'][0:10]
for idx, issue in enumerate(issues, start=1):
    print(f"{idx}: {issue}")

# Formatierung der Beschwerden, um den Wortschatz zu erstellen
complaints = [' '.join(row) for row in df['Issue']]
complaints_df = pd.DataFrame({'Issue': complaints})

# Überprüfen, ob es leere Dokumente gibt
if len(complaints) == 0:
    raise ValueError("Alle Dokumente sind leer nach der Bereinigung.")

# 1. Vektorisierung / Erstellung des Bag of Words mit CountVectorizer 
vect = CountVectorizer()
data = vect.fit_transform(complaints)
data_df = pd.DataFrame(data.toarray(), columns=vect.get_feature_names_out())

# 2. Vektorisierung / Erstellung des TF-IDF-Modells mit TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=1)
data_vectorized = vectorizer.fit_transform(complaints)
data_TF_IDF = pd.DataFrame(data_vectorized.toarray(), columns=vectorizer.get_feature_names_out())

# Häufigste Wörter im Bag of Words
bow_word_counts = data_df.sum().sort_values(ascending=False).head(20)
print("\n\nHäufigste Wörter im Bag of Words:")
print(bow_word_counts)

# Wörter mit den höchsten TF-IDF-Werten
tfidf_word_scores = data_TF_IDF.sum().sort_values(ascending=False).head(20)
print("\n\nWörter mit den höchsten TF-IDF-Werten:")
print(tfidf_word_scores)

# Erstellen der Subplots
ig, axes = plt.subplots(1, 2, figsize=(14, 7))

# Plot für Bag of Words
axes[0].barh(bow_word_counts.index[::-1], bow_word_counts.values[::-1])
axes[0].set_title('Häufigste Wörter (Bag of Words)')
axes[0].set_xlabel('Häufigkeit')
axes[0].set_ylabel('Wörter')

# Plot für TF-IDF
axes[1].barh(tfidf_word_scores.index[::-1], tfidf_word_scores.values[::-1])
axes[1].set_title('Häufigste Wörter (TF-IDF)')
axes[1].set_xlabel('Wert')
axes[1].set_ylabel('Wörter')

# Gensim Dictionary und Corpus erstellen
dictionary = Dictionary(df['Issue'])
corpus = [dictionary.doc2bow(text) for text in df['Issue']]

#LDA durchführen und formatieren
n_components_lda = 5 # Anzahl der Themen
n_top_words_lda = 7 # Anzahl der Top-Wörter, die für jedes Topic angezeigt werden sollen
lda_model = LatentDirichletAllocation(n_components=n_components_lda, random_state=42)
lda_model.fit_transform(data_vectorized)
lda_topics = lda_model.components_

 # Funktion zum Formatieren der Topics & Wahrscheinlichkeiten
def format_topics(topics, feature_names, model_name, n_top_words):
    headers_words = ["Wort " + str(i+1) for i in range(n_top_words)]
    topics_words = []
       
    for topic in topics:
        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]
        topics_words.append([feature_names[i] for i in top_words_indices])
            
    topics_df = pd.DataFrame(topics_words, columns=headers_words)
    topics_df.index = [f"Topic {i+1}" for i in range(len(topics))]
    
    print(f"\n{model_name} Topics (Wörter):")
    print(tabulate(topics_df, headers='keys', tablefmt='pretty'))


# Aufruf der Funktion mit angepassten Indizes
feature_names = vectorizer.get_feature_names_out()
format_topics(lda_topics, feature_names, "LDA", n_top_words=n_top_words_lda)

# Kohärenzwert für LDA-Modell berechnen
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_components_lda, random_state=42, update_every=1, passes=10, alpha='auto', per_word_topics=True)
coherence_model_lda = CoherenceModel(model=lda_model, texts=df['Issue'], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print(f"\nKohärenzwert des LDA-Modells: {coherence_lda}")


# Berechnung der Kohärenzwerte für jedes Topic
coherence_lda_topic = coherence_model_lda.get_coherence_per_topic()
print('Kohärenzwerte pro Topic: ', coherence_lda_topic, '\n')


# PyLDAvis-Visualisierung - LDA-Modell mit Gensim erstellen
pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, dictionary)
display(vis)


# LSA-Modell (TruncatedSVD)
n_top_words_lsa = 7 # Anzahl der Top-Wörter, die für jedes Topic angezeigt werden sollen
n_components_lsa = 5  # Anzahl der Hauptkomponenten
lsa_model = TruncatedSVD(n_components=n_components_lsa, algorithm='randomized', random_state=42)
lsa_top = lsa_model.fit_transform(data_vectorized)
lsa_topics = lsa_model.components_

# Wörter und Wahrscheinlichkeiten für jedes LSA-Topic extrahieren
def get_lsa_topic_words(lsa_model, feature_names, n_top_words):
    topics_words = []
    for topic_idx, topic in enumerate(lsa_model.components_):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        topics_words.append([feature_names[i] for i in top_indices])
    return topics_words


# Funktion zur Formatierung der Topics
def format_topics_lsa(topics_words, lsa_model, model_name, n_top_words):
    headers_words = [f"Wort {i+1}" for i in range(n_top_words)]
       
    topics_df = pd.DataFrame(topics_words, columns=headers_words[:len(topics_words[0])])
    topics_df.index = [f"Topic {i+1}" for i in range(len(topics_words))]
    
    print(f"\n{model_name} Topics (Wörter):")
    print(tabulate(topics_df, headers='keys', tablefmt='pretty'))
       
    return topics_df

# Aufruf
feature_names = vectorizer.get_feature_names_out()
topics_words = get_lsa_topic_words(lsa_model, feature_names, n_top_words=n_top_words_lsa )
format_topics_lsa(topics_words, lsa_model, model_name="LSA", n_top_words=n_top_words_lsa)


# Funktion zur Berechnung Kohärenzwert für LSA-Modell berechnen
def calculate_coherence_lsa(lsa_model, corpus, texts, dictionary, n_topics, n_top_words):
    topics = lsa_model.components_
    top_words = []
    
    for topic in topics:
        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words.append([dictionary[i] for i in top_words_indices])
    
    coherence_model_lsa = CoherenceModel(topics=top_words, texts=df['Issue'], dictionary=dictionary, coherence='c_v')
    coherence_lsa = coherence_model_lsa.get_coherence()
    return coherence_lsa


# Funktion zur Berechnung der Kohärenz pro Topic für LSA
def calculate_coherence_per_topic_lsa(lsa_model, corpus, texts, dictionary, n_topics, n_top_words):
    topics = lsa_model.components_
    top_words = []
    
    for topic in topics:
        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words.append([dictionary[i] for i in top_words_indices])
    
    coherence_lsa_topic = []
    
    for i, topic_words in enumerate(top_words):
        coherence_model_topic = CoherenceModel(topics=[topic_words], texts=df['Issue'], dictionary=dictionary, coherence='c_v')
        coherence_topic = coherence_model_topic.get_coherence()
        coherence_lsa_topic.append(coherence_topic)

    print('Kohärenzwerte pro Topic: ', coherence_lsa_topic)
    return coherence_lsa_topic


# Corpus in das richtige Format für Gensim konvertieren
corpus_sparse = csr_matrix(lsa_top)  # Konvertiere numpy.ndarray in eine sparse Matrix
corpus_gensim = Sparse2Corpus(corpus_sparse, documents_columns=False)

# Kohärenzwert für LSA ausgeben
coherence_lsa = calculate_coherence_lsa(lsa_model, corpus, df['Issue'], dictionary, n_topics=n_components_lsa, n_top_words=n_top_words_lsa)
print('\nLSA: ')
print(f"Kohärenzwert des LSA-Modells: {coherence_lsa}")
tokenized_docs = df['Issue'].tolist() 
coherence_values_lsa = calculate_coherence_per_topic_lsa(lsa_model, corpus_gensim, tokenized_docs, dictionary, n_topics=n_components_lsa, n_top_words=n_top_words_lsa)


# Visualisierung der Topics für LSA
def visualize_topics_lsa(topics, feature_names, n_top_words):
    headers = [f"Wort {i+1}" for i in range(n_top_words)]
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Für jedes Topic ein Balkendiagramm erstellen
    for topic_idx, topic in enumerate(topics):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        top_weights = [topic[i] for i in top_indices]
        
        # Balkendiagramm für jedes Topic erstellen
        ax.barh(top_words, top_weights, label=f"Topic {topic_idx+1}", alpha=0.8)
    
    ax.set_xlabel('Gewichtung')
    ax.set_title('Top-Wörter je Topic (LSA)')
    ax.legend()
    plt.gca().invert_yaxis()  # Invertieren der y-Achse für bessere Lesbarkeit
    plt.tight_layout()
    plt.show()

# Aufruf für die Visualisierung der Topics
visualize_topics_lsa(lsa_topics, feature_names, n_top_words=n_top_words_lsa)


# Optimiertes LDA-Modell
## Definition Parameter
coherence_measures = ['c_v']
num_topics_list = (2, 10, 2) 
random_states = [42, None]
alphas = ['symmetric', 'asymmetric', 'auto']
etas = [None, 'auto'] 
passes = [10]
update_every = [1]

## Definition einer Funktion um Kohärenzwerte für verschiedene LDA-Modelle zu berechnen
def optimize_lda_coherence(dictionary, corpus, texts, coherence_measures, num_topics_range, random_states, alphas, etas, passes, update_every):
    coherence_values = []
    model_list = []
    parameter_combinations = list(itertools.product(random_states, alphas, etas, passes, update_every))
    num_topics_start, num_topics_end, num_topics_step = num_topics_range
    
    print("\n\nOptimiertes LDA:")

    for num_topics in range(num_topics_start, num_topics_end + 1, num_topics_step):
        for params in parameter_combinations:
            random_state, alpha, eta, pass_num, update_every_val = params
            
            # Create and train the LDA model
            model = LdaModel(
                corpus=corpus,
                id2word=dictionary,
                num_topics=num_topics,
                random_state=random_state,
                alpha=alpha,
                eta=eta,
                passes=pass_num,
                update_every=update_every_val,
                per_word_topics=True
            )
            model_list.append((model, params))
            
            for coherence_meausure in coherence_measures:
                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence=coherence_meausure)
                coherence_score = coherencemodel.get_coherence()
                coherence_values.append((coherence_score, num_topics, coherence_meausure, params))
                # Ausgabe der Ergebnisse für jede Kombination
                print(f"Anzahl Topics = {num_topics}, random_state = {random_state}, alpha = {alpha}, eta = {eta}, passes = {pass_num}, update_every = {update_every_val}, Kohärenzmaß = {coherence_meausure} ist der Kohärenzwert: {coherence_score}")
            

    return model_list, coherence_values


# Aufruf der Funktion mit den vorgegebenen Parametern
model_list, coherence_values = optimize_lda_coherence(
    dictionary, corpus, tokenized_docs, 
    coherence_measures=coherence_measures, 
    num_topics_range=num_topics_list,
    random_states=random_states, 
    alphas=alphas, 
    etas=etas, 
    passes=passes, 
    update_every=update_every
)

# Ermittlung und Ausgabe der besten Kohärenzwert und der zugehörigen Parameter
best_coherence = max(coherence_values, key=lambda x: x[0])
best_coherence_value, best_num_topics, best_coherence_measure, best_params = best_coherence

print(f"\nBester Kohärenz-Wert: {best_coherence_value}")
print(f"Beste Anzahl an Themen: {best_num_topics}")
print(f"Beste Parameter: coherence= {best_coherence_measure}, random_state = {best_params[0]}, alpha = {best_params[1]}, eta = {best_params[2]}, passes = {best_params[3]}, update_every = {best_params[4]}")


## Optimierte Werte für LSA finden
# Definition der Parameterkombinationen
n_components_list = [2, 3, 5]
algorithm_list = ['arpack', 'randomized']
n_iter_list = [5, 10]
ltol_list = [0.0, 0.01] # Mögliche Werte z.B [0.0, 0.01, 0.001]
random_state_list = [42, None] # Mögliche Werte [42, None, 0]
n_top_words_lsa_list = [5, 10]


# Platzhalter für beste Parameter und besten Kohärenzwert
best_coherence = -1
best_parameters = None

# Ausgabe der besten Parameter und des besten Kohärenzwerts
print("\n\nOptimiertes LSA:")

# Durchlauf über die Parameterkombinationen
for n_components in n_components_list:
    for algorithm in algorithm_list:
        for random_state in random_state_list:
            for n_iter in n_iter_list:
                for tol in ltol_list:
                    for n_top_words in n_top_words_lsa_list:
                        lsa_model = TruncatedSVD(n_components=n_components, algorithm=algorithm, random_state=random_state, n_iter=n_iter, tol=tol)
                        lsa_model.fit(data_vectorized)
                        coherence_lsa = calculate_coherence_lsa(lsa_model, corpus, df['Issue'], dictionary, n_topics=n_components, n_top_words=n_top_words)
                        print(f"Anzahl Topics = {n_components}, algorithm = {algorithm}, random_state = {random_state}, n_iter = {n_iter}, tol = {tol}, n_top_words = {n_top_words} ist der Kohärenzwert: {coherence_lsa}")
                        
                        # Wenn der aktuelle Kohärenzwert besser ist als der bisher beste, aktualisiere
                        if coherence_lsa > best_coherence:
                            best_coherence = coherence_lsa
                            best_parameters = {
                                'n_components': n_components, 
                                'algorithm': algorithm, 
                                'random_state': random_state,
                                'n_iter': n_iter,
                                'tol': tol,
                                'n_top_words': n_top_words
                            }

print("\nBeste Parameter:")
print(best_parameters)
print(f"Beste Kohärenz: {best_coherence}")

## KeyBert
# Extraktion der Keywords für jede Beschwerde und Speicherung in einer neuen Spalte 'Keywords'
kw_model = KeyBERT()
complaints_df['Keywords'] = complaints_df['Issue'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 1)))

# Extrahieren und Transformieren der Keywords
keywords_flat = [keyword for sublist in complaints_df['Keywords'].tolist() for keyword, _ in sublist]

# Berechnung der Häufigkeit der Schlüsselwörter
keyword_freq = pd.Series(keywords_flat).value_counts().head(10)

# Anzeige der extrahierten Schlüsselwörter für die ersten 10 Beschwerden
print("\nExtrahierte Schlüsselwörter für die ersten 10 Beschwerden:")
for idx, row in complaints_df.head(10).iterrows():
    keywords_list = ", ".join([f"({kw}, {score:.4f})" for kw, score in row['Keywords']])
    print(f"{idx + 1}  {row['Issue']: <30} {keywords_list}")


# Visualisierung der häufigsten Schlüsselwörter
sns.set_palette('pastel')
plt.figure(figsize=(10, 6))
sns.barplot(x=keyword_freq.values, y=keyword_freq.index)
plt.title('Häufigste Schlüsselwörter (KeyBERT)')
plt.xlabel('Häufigkeit')
plt.ylabel('Schlüsselwort')
plt.tight_layout()
plt.show()

